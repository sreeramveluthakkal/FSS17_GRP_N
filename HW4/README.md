# FSS17_GRP_N
## HW4 - Regression Trees

### Description
Building a regression tree learner:

Apply supervised Discretization
At each level of the tree, break the data on the ranges and find the column whose breaks most reduces the variability of the target variable (use dom).  
For each break, apply the regression tree learner recursively.  
Recursion stops when the breaks do not improve the supervised target score, when there are too few examples to break, or when the tree depth is too much.
Write a list printer that recurses down the tree and prints details about each node, indented by its level in tree.  

Test: run your decision tree learner on auto.csv. Using dom and tooFew is 10, the auto.csv divides into something like this:

### Files
`d.py` - python code for HW4   
`output.txt` - Table generated by the code in b.py sorted by domination rank  

### Dependencies
`Python 2.7`

### Building
`python d.py <inputfile> <small value> <usedom> <tooFew>`   
The arguments are described below.  

### Arguments
`<input file>` is input file name in __comma separated format with headers__.  
`<small value>` is cohen value and is a __value <= 1__.  
`<usedom>` is a __boolean value of 0 or 1__ indicating weather domination rank is to be used for combining bins in supervised learning. `1` indicates that domination rank is to used. If indicated otherwise, it will use the last column (assumed to be num)  
`<tooFew>` an integer which sets the recursion end conditions based on the number of examples to break.   

### Sample Output
__COMMAND__: `python d.py auto.csv 0.2 1 10`  
__DESCRIPTION__: This is the test case given in the question.  
- run the decision tree learner on auto.csv  
- with cohen value as 0.2  
- using dom to find breaks at each level of the tree and  
- using tooFew as 10 as the minimum number of examples needed at each level.  
__OUTPUT__:  

### Contributors
Aswin Anil Kumar,  
Seyedsamim Mirhosseini Ghamsari,  
Sreeram Veluthakkal
